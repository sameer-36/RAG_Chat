# ğŸ“„ Chat with PDFs Locally using Ollama & LangChain

Interact with your PDF documents using local embeddings and LLMs. This app uses [LangChain](https://www.langchain.com/), [FAISS](https://github.com/facebookresearch/faiss), and [Ollama](https://ollama.com/) to let you ask natural language questions about the contents of your uploaded PDFs â€” all locally, with no cloud or OpenAI dependency.

![Streamlit App Screenshot](preview.png) <!-- Optional: Replace with an actual screenshot -->

---

## ğŸš€ Features

- ğŸ“¤ Upload and process multiple PDF files
- ğŸ§  Create and manage a FAISS vector store locally
- ğŸ’¬ Ask natural language questions about your PDFs
- ğŸ§¾ View previous questions and answers
- âš™ï¸ Full local setup (no API keys or cloud calls required)
- ğŸª„ Powered by Ollama + DeepSeek LLM + LangChain

---

## ğŸ§© Technologies Used

- [Streamlit](https://streamlit.io/) â€” UI Framework
- [LangChain](https://www.langchain.com/) â€” LLM and QA Chain
- [FAISS](https://github.com/facebookresearch/faiss) â€” Local vector store
- [Ollama](https://ollama.com/) â€” Local LLM and embeddings
- [PyPDFLoader](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf) â€” PDF content loader

---

## ğŸ–¥ï¸ Installation

1. **Clone the Repository**

```bash
git clone https://github.com/yourusername/pdf-chat-local.git
cd pdf-chat-local
# RAG_Chat

    Create a Virtual Environment (optional but recommended)

python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

    Install Dependencies

pip install -r requirements.txt

    Install and Run Ollama

    You must have Ollama installed and a compatible model (e.g., deepseek-r1:1.5b) pulled locally.

ollama pull deepseek-r1:1.5b

    Run the App

streamlit run app.py

ğŸ“ Project Structure

â”œâ”€â”€ app.py                  # Main Streamlit application
â”œâ”€â”€ data/                   # Uploaded PDFs will be stored here
â”œâ”€â”€ faiss_index/            # Vector index folder (auto-created)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸ§  Example Usage

    Upload your PDF files using the file uploader.

    Click "âš™ï¸ Process Documents" to generate embeddings and build the vector store.

    Ask questions in natural language â€” e.g., "What is the main topic of the document?" or "Summarize section 3."

    View previous questions and answers.

âš ï¸ Notes

    All operations are local â€” no data is sent to any external APIs.

    This app assumes deepseek-r1:1.5b is available locally via Ollama.

    FAISS index and uploaded files persist unless cleared via the UI.

ğŸ“œ License

MIT License. See LICENSE for more information.
ğŸ™Œ Acknowledgements

    LangChain

    Ollama

    Streamlit

âœ¨ Future Enhancements

    PDF text preview and search

    Dark mode toggle

    Model selection dropdown

    Export chat history


---

### âœ… Bonus: `requirements.txt`

If you havenâ€™t already, include this:

```txt
streamlit
langchain
faiss-cpu
langchain-community
langchain-ollama
pypdf
